{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IKOICHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x)\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my Softmax Function\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    sm = ez / np.sum(ez)\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function associated with Softmax, the cross-entropy loss, is:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.\n",
    ">**Recall:** In this course, Loss is for one example while Cost covers all examples. \n",
    " \n",
    " \n",
    "Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Now the cost is:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IKOICHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating Model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(25, activation='relu', name='layer1'),\n",
    "        Dense(15, activation='relu', name='layer2'),\n",
    "        Dense(4 , activation='sigmoid', name='layer3')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\IKOICHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "63/63 [==============================] - 7s 10ms/step - loss: 0.9484\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.4476\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2549\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1541\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1056\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0812\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0676\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0591\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0533\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19bc6c16a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 7ms/step\n",
      "[[0.34081835 0.05275655 0.9472803  0.30914465]\n",
      " [0.99606645 0.6663822  0.06256275 0.011466  ]]\n",
      "largest value 0.999976 smallest value 3.8892226e-06\n"
     ]
    }
   ],
   "source": [
    "p_nonpreferred = model.predict(X_train)\n",
    "print(p_nonpreferred [:2])\n",
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34081835 0.05275655 0.9472803  0.30914465], category: 2\n",
      "[0.99606645 0.6663822  0.06256275 0.011466  ], category: 0\n",
      "[0.98589414 0.71075076 0.10981676 0.02571319], category: 0\n",
      "[0.54264534 0.9883067  0.04006017 0.1688556 ], category: 1\n",
      "[0.68765503 0.00207645 0.99371547 0.04023519], category: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print( f\"{p_nonpreferred[i]}, category: {np.argmax(p_nonpreferred[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   #<-- Note\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 4s 11ms/step - loss: 1.4640\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5929\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2803\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1519\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1004\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0764\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0635\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0557\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0505\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19bc8161950>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferred_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 8ms/step\n",
      "two example output vectors:\n",
      " [[-1.0803182  -2.685932    3.2586536  -0.05245831]\n",
      " [ 5.513552    0.45815724 -4.297861   -3.355855  ]]\n",
      "largest value 10.480713 smallest value -10.253984\n"
     ]
    }
   ],
   "source": [
    "p_preferred = preferred_model.predict(X_train)\n",
    "print(f\"two example output vectors:\\n {p_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output predictions are not probabilities!\n",
    "If the desired output are probabilities, the output should be be processed by a softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two example output vectors:\n",
      " [[1.2403165e-02 2.4901382e-03 9.5043892e-01 3.4667820e-02]\n",
      " [9.9347258e-01 6.3332357e-03 5.4464486e-05 1.3970793e-04]]\n",
      "largest value 0.9999896 smallest value 3.464973e-08\n"
     ]
    }
   ],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the most likely category, the softmax is not required. One can find the index of the largest output using np.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0803182  -2.685932    3.2586536  -0.05245831], category: 2\n",
      "[ 5.513552    0.45815724 -4.297861   -3.355855  ], category: 0\n",
      "[ 4.1857724  0.6524269 -3.239903  -2.8281155], category: 0\n",
      "[-2.004035   3.3781226 -1.2430665 -2.4520552], category: 1\n",
      "[ 1.4730426 -3.3420036  6.623793  -1.6249774], category: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
