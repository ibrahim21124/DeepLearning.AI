{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IKOICHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "\n",
    "A multiclass neural network generates N outputs. One output is selected as the predicted answer. In the output layer, a vector $\\mathbf{z}$ is generated by a linear function which is fed into a softmax function.\n",
    "\n",
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=0}^{N-1}{e^{z_k} }} \\tag{1}$$\n",
    "\n",
    "Where $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ and N is the number of feature/categories in the output layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    '''\n",
    "    Softmax Converts Vectors of Values to Probability\n",
    "\n",
    "    Args:\n",
    "        z (ndarray (N, )) : Input data, N features\n",
    "    Returns:\n",
    "        a (ndarray (N,)) : Softmax of z\n",
    "    '''\n",
    "    exp_z = np.exp(z)\n",
    "    a = exp_z / np.sum(exp_z)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = np.load(\"X.npy\")\n",
    "    y = np.load(\"y.npy\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y  = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (5000, 400)\n",
      "The shape of y is: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X is: ' + str(x.shape))\n",
    "print ('The shape of y is: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [                       \n",
    "        tf.keras.Input(shape=(400,)),\n",
    "        Dense(25, activation='relu', name='L1'),\n",
    "        Dense(15, activation='relu', name='L2'),\n",
    "        Dense(10, activation='linear', name='L3')\n",
    "\n",
    "    ], name = \"my_model\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " L1 (Dense)                  (None, 25)                10025     \n",
      "                                                                 \n",
      " L2 (Dense)                  (None, 15)                390       \n",
      "                                                                 \n",
      " L3 (Dense)                  (None, 10)                160       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10575 (41.31 KB)\n",
      "Trainable params: 10575 (41.31 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape = (400, 25), b1 shape = (25,)\n",
      "W2 shape = (25, 15), b2 shape = (15,)\n",
      "W3 shape = (15, 10), b3 shape = (10,)\n"
     ]
    }
   ],
   "source": [
    "[layer1, layer2, layer3] = model.layers\n",
    "W1,b1 = layer1.get_weights()\n",
    "W2,b2 = layer2.get_weights()\n",
    "W3,b3 = layer3.get_weights()\n",
    "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
    "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
    "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "157/157 [==============================] - 4s 6ms/step - loss: 1.4086\n",
      "Epoch 2/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.5308\n",
      "Epoch 3/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.4000\n",
      "Epoch 4/40\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 0.3332\n",
      "Epoch 5/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.2955\n",
      "Epoch 6/40\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 0.2665\n",
      "Epoch 7/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.2433\n",
      "Epoch 8/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.2220\n",
      "Epoch 9/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.2093\n",
      "Epoch 10/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1887\n",
      "Epoch 11/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.1799\n",
      "Epoch 12/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.1686\n",
      "Epoch 13/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.1604\n",
      "Epoch 14/40\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 0.1503\n",
      "Epoch 15/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1385\n",
      "Epoch 16/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1365\n",
      "Epoch 17/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.1258\n",
      "Epoch 18/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1204\n",
      "Epoch 19/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1114\n",
      "Epoch 20/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1075\n",
      "Epoch 21/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1021\n",
      "Epoch 22/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0992\n",
      "Epoch 23/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0930\n",
      "Epoch 24/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0874\n",
      "Epoch 25/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0810\n",
      "Epoch 26/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0790\n",
      "Epoch 27/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0764\n",
      "Epoch 28/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0726\n",
      "Epoch 29/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0675\n",
      "Epoch 30/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0681\n",
      "Epoch 31/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0588\n",
      "Epoch 32/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0548\n",
      "Epoch 33/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0559\n",
      "Epoch 34/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0506\n",
      "Epoch 35/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0471\n",
      "Epoch 36/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0474\n",
      "Epoch 37/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0438\n",
      "Epoch 38/40\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0390\n",
      "Epoch 39/40\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0387\n",
      "Epoch 40/40\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 0.0357\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x, y,\n",
    "    epochs = 40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 474ms/step\n",
      " predicting a Two: \n",
      "[[-10.939243    2.51797     6.054313    2.427497  -11.3835335  -7.2253385\n",
      "   -6.3267775   4.152692   -6.6699986  -6.2704263]]\n",
      " Largest Prediction index: 2\n"
     ]
    }
   ],
   "source": [
    "image_of_two = x[1015]\n",
    "\n",
    "prediction = model.predict(image_of_two.reshape(1,400))  # prediction\n",
    "\n",
    "print(f\" predicting a Two: \\n{prediction}\")\n",
    "print(f\" Largest Prediction index: {np.argmax(prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicting a Two. Probability vector: \n",
      "[[3.4576676e-08 2.4164453e-02 8.2983410e-01 2.2074198e-02 2.2173309e-08\n",
      "  1.4181163e-06 3.4829893e-06 1.2391607e-01 2.4711146e-06 3.6848930e-06]]\n",
      "Total of predictions: 1.000\n"
     ]
    }
   ],
   "source": [
    "prediction_p = tf.nn.softmax(prediction)\n",
    "\n",
    "print(f\" predicting a Two. Probability vector: \\n{prediction_p}\")\n",
    "print(f\"Total of predictions: {np.sum(prediction_p):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.argmax(prediction_p): 2\n"
     ]
    }
   ],
   "source": [
    "yhat = np.argmax(prediction_p)\n",
    "\n",
    "print(f\"np.argmax(prediction_p): {yhat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
